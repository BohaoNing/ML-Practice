{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ae8ca6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003895,
     "end_time": "2022-10-29T23:44:30.112659",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.108764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. What is the fundamental idea behind support vector machines?\n",
    "        \n",
    "        The goal is to have the largest possible margin between the decision boundary that separates the two classes and the training instances. \n",
    "        When performing soft margin classification, the SVM searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street).\n",
    "        Another key idea is to use kernels when training on nonlinear datasets. \n",
    "        SVMs can also be tweaked to perform linear and nonlinear regression, as well as novelty detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17b9b52",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002773,
     "end_time": "2022-10-29T23:44:30.118439",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.115666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. What is a support vector?\n",
    "        \n",
    "        The points on (and inside) the decision boundaries.\n",
    "        A support vector is any instance located on the \"street\", including its border. The decision boundary is entirely determined by the support vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb426d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002553,
     "end_time": "2022-10-29T23:44:30.123715",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.121162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. Why is it important to scale the inputs when using SVMs? \n",
    "        \n",
    "        Because they are sensitive to feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae6342",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00253,
     "end_time": "2022-10-29T23:44:30.129421",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.126891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. Can an SVM classiﬁer output a conﬁdence score when it classiﬁes an instance? What about a probability?\n",
    "        \n",
    "        You can use the decision_function() method to get confidence scores. These scores represent the distance between the instance and the decision boundary.\n",
    "        If you use the SVC class instead of LinearSVC, and if you set its probability hyperparameter to True, then the model will ﬁt an extra model at the end of training to map the SVM decision function scores to estimated probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de29ac1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002565,
     "end_time": "2022-10-29T23:44:30.134765",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.132200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "5. How can you choose between LinearSVC, SVC, and SGDClassifier?\n",
    "\n",
    "        They have different time complexity, out-of-core support, scaling requirement and kernel trick.\n",
    "        SGDClassifier is more flexible, plus it supports incremental learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4949a5d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002506,
     "end_time": "2022-10-29T23:44:30.140033",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.137527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "6. Say you’ve trained an SVM classiﬁer with an RBF kernel, but it seems to underﬁt the training set. Should you increase or decrease γ (gamma)? What about C?\n",
    "\n",
    "        Increase gamma and increase C."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1d3295",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002566,
     "end_time": "2022-10-29T23:44:30.145979",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.143413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "7. What does it mean for a model to be ϵ-insensitive?\n",
    "\n",
    "        Reducing ϵ increases the number of support vectors, which regularizes the model. Moreover, if you add more training instances within the margin, it will not aﬀect the model’s predictions; thus, the model is said to be ϵ-insensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16270d4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00258,
     "end_time": "2022-10-29T23:44:30.151339",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.148759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "8. What is the point of using the kernel trick?\n",
    "\n",
    "        The kernel trick is mathematical technique that makes it possible to train a nonlinear SVM model.\n",
    "        The resulting model is equivalent to mapping the inputs to another space using a nonlinear transformation, then training a linear SVM on the resulting high-dimensional inputs.\n",
    "        The kernel trick gives the same result without having to transform the inputs at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea0316",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002517,
     "end_time": "2022-10-29T23:44:30.156554",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.154037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "9. Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91055429",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002533,
     "end_time": "2022-10-29T23:44:30.161847",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.159314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "10. Train an SVM classiﬁer on the wine dataset, which you can load using sklearn.datasets.load_wine(). This dataset contains the chemical analyses of 178 wine samples produced by 3 diﬀerent cultivators: the goal is to train a classiﬁcation model capable of predicting the cultivator based on the wine’s chemical analysis. Since SVM classiﬁers are binary classiﬁers, you will need to use one-versus-all to classify all three classes. What accuracy can you reach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef007dc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002504,
     "end_time": "2022-10-29T23:44:30.167144",
     "exception": false,
     "start_time": "2022-10-29T23:44:30.164640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "11. Train and ﬁne-tune an SVM regressor on the California housing dataset. You can use the original dataset rather than the tweaked version we used in Chapter 2, which you can load using sklearn.datasets.fetch_california_housing(). The targets represent hundreds of thousands of dollars. Since there are over 20,000 instances, SVMs can be slow, so for hyperparameter tuning you should use far fewer instances (e.g., 2,000) to test many more hyperparameter combinations. What is your best model’s RMSE?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.695724,
   "end_time": "2022-10-29T23:44:30.793822",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-10-29T23:44:21.098098",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
