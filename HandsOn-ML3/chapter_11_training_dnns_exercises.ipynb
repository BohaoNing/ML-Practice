{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32b4dc1",
   "metadata": {
    "papermill": {
     "duration": 0.003143,
     "end_time": "2022-12-04T16:13:58.668441",
     "exception": false,
     "start_time": "2022-12-04T16:13:58.665298",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. What is the problem that Glorot initialization and He initialization aim to ﬁx?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7cbd9",
   "metadata": {
    "papermill": {
     "duration": 0.00183,
     "end_time": "2022-12-04T16:13:58.672609",
     "exception": false,
     "start_time": "2022-12-04T16:13:58.670779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3375546a",
   "metadata": {
    "papermill": {
     "duration": 0.001825,
     "end_time": "2022-12-04T16:13:58.676480",
     "exception": false,
     "start_time": "2022-12-04T16:13:58.674655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. Is it OK to initialize the bias terms to 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585147d",
   "metadata": {
    "papermill": {
     "duration": 0.001809,
     "end_time": "2022-12-04T16:13:58.680383",
     "exception": false,
     "start_time": "2022-12-04T16:13:58.678574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. In which cases would you want to use each of the activation functions we discussed in this chapter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7560d3",
   "metadata": {
    "papermill": {
     "duration": 0.001746,
     "end_time": "2022-12-04T16:13:58.684106",
     "exception": false,
     "start_time": "2022-12-04T16:13:58.682360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c671c3e",
   "metadata": {
    "papermill": {
     "duration": 0.001781,
     "end_time": "2022-12-04T16:13:58.687989",
     "exception": false,
     "start_time": "2022-12-04T16:13:58.686208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "6. Name three ways you can produce a sparse model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80708df8",
   "metadata": {
    "papermill": {
     "duration": 0.001793,
     "end_time": "2022-12-04T16:13:58.691840",
     "exception": false,
     "start_time": "2022-12-04T16:13:58.690047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC dropout?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dabb63",
   "metadata": {
    "papermill": {
     "duration": 0.001886,
     "end_time": "2022-12-04T16:13:58.695699",
     "exception": false,
     "start_time": "2022-12-04T16:13:58.693813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "    a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function.\n",
    "\n",
    "    b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with tf.keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "\n",
    "    c. Now try adding batch normalization and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it aﬀect training speed?\n",
    "\n",
    "    d. Try replacing batch normalization with SELU, and make the necessary adjustments to ensure the network selfnormalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "    e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC dropout. f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.541338,
   "end_time": "2022-12-04T16:13:59.319693",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-04T16:13:50.778355",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
