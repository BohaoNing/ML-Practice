{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14967f69",
   "metadata": {
    "papermill": {
     "duration": 0.003743,
     "end_time": "2022-11-12T00:35:10.945595",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.941852",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. How would you deﬁne clustering? Can you name a few clustering algorithms?\n",
    "\n",
    "    Grouping similar instances together. K-means, DBSCAN, Gaussian mixture model.\n",
    "    \n",
    "    The notion of similarity depends on the task at hand: for example, in some cases two nearby instances will be considered similar, while in others similar instances may be far apart as long as they belong to the same densely packed group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddec6f6",
   "metadata": {
    "papermill": {
     "duration": 0.002472,
     "end_time": "2022-11-12T00:35:10.951087",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.948615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. What are some of the main applications of clustering algorithms?\n",
    "\n",
    "    Data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, anomaly detection, and novelty detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a1371",
   "metadata": {
    "papermill": {
     "duration": 0.002374,
     "end_time": "2022-11-12T00:35:10.956172",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.953798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. Describe two techniques to select the right number of clusters when using k-means.\n",
    "\n",
    "    Plot the inertia as a function of k;\n",
    "    silhouette_score()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851b59b",
   "metadata": {
    "papermill": {
     "duration": 0.002411,
     "end_time": "2022-11-12T00:35:10.961193",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.958782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. What is label propagation? Why would you implement it, and how?\n",
    "\n",
    "    Labeling a dataset is costly and time-consuming. Therefore, it is common to have plenty of unlabeled instances, but few labeled instances.\n",
    "    \n",
    "    Label propagation is a technique that consists in copying some (or all) of the labels from the labeled instances to similar unlabeled instances. This can greatly extend the number of labeled instances, and thereby allow a supervised algorithm to reach better performance (this is a form of semi-supervised learning).\n",
    "    \n",
    "    One approach is to use a clustering algorithm such as K-Means on all the instances, then for each cluster find the most common label or the label of the most representative instance (i.e., the one closest to the centroid) and propagate it to the unlabeled instances in the same cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764617d",
   "metadata": {
    "papermill": {
     "duration": 0.003015,
     "end_time": "2022-11-12T00:35:10.966856",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.963841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "5. Can you name two clustering algorithms that can scale to large datasets? And two that look for regions of high density?\n",
    "\n",
    "    Scale to large datasets: k-means, BIRCH\n",
    "    \n",
    "    High density: DBSCAN, mean-shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686eea0",
   "metadata": {
    "papermill": {
     "duration": 0.002491,
     "end_time": "2022-11-12T00:35:10.972381",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.969890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "6. Can you think of a use case where active learning would be useful? How would you implement it?\n",
    "\n",
    "    Active learning is useful whenever you have plenty of unlabeled instances but labeling is costly. In this case (which is very common), rather than randomly selecting instances to label, it is often preferable to perform active learning, where human experts interact with the learning algorithm, providing labels for specific instances when the algorithm requests them. A common approach is uncertainty sampling (see the Active Learning section in chapter 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682569e",
   "metadata": {
    "papermill": {
     "duration": 0.00258,
     "end_time": "2022-11-12T00:35:10.977807",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.975227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "7. What is the diﬀerence between anomaly detection and novelty detection?\n",
    "\n",
    "    In novelty detection, the dataset is assumed to be \"clean\", without outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5889326",
   "metadata": {
    "papermill": {
     "duration": 0.002491,
     "end_time": "2022-11-12T00:35:10.983128",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.980637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "8. What is a Gaussian mixture? What tasks can you use it for?\n",
    "\n",
    "    A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. In other words, the assumption is that the data is grouped into a finite number of clusters, each with an ellipsoidal shape (but the clusters may have different ellipsoidal shapes, sizes, orientations, and densities), and we don't know which cluster each instance belongs to. \n",
    "    \n",
    "    This model is useful for density estimation, clustering, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b18a40",
   "metadata": {
    "papermill": {
     "duration": 0.002431,
     "end_time": "2022-11-12T00:35:10.988313",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.985882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "9. Can you name two techniques to ﬁnd the right number of clusters when using a Gaussian mixture model?\n",
    "\n",
    "    Find the model that minimizes a theoretical information criterion such as BIC and AIC;\n",
    "    \n",
    "    Use BayesianGaussianMixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c9360",
   "metadata": {
    "papermill": {
     "duration": 0.002489,
     "end_time": "2022-11-12T00:35:10.993455",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.990966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "10. The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images of faces. Each image is ﬂattened to a 1D vector of size 4,096. Forty diﬀerent people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Load the dataset using the sklearn.datasets.fetch_olivetti_faces() function, then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you will probably want to use stratiﬁed sampling to ensure that there are the same number of images per person in each set. Next, cluster the images using k-means, and ensure that you have a good number of clusters (using one of the techniques discussed in this chapter). Visualize the clusters: do you see similar faces in each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6673d4e9",
   "metadata": {
    "papermill": {
     "duration": 0.002374,
     "end_time": "2022-11-12T00:35:10.998439",
     "exception": false,
     "start_time": "2022-11-12T00:35:10.996065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "11. Continuing with the Olivetti faces dataset, train a classiﬁer to predict which person is represented in each picture, and evaluate it on the validation set. Next, use k-means as a dimensionality reduction tool, and train a classiﬁer on the reduced set. Search for the number of clusters that allows the classiﬁer to get the best performance: what performance can you reach? What if you append the features from the reduced set to the original features (again, searching for the best number of clusters)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02710f2",
   "metadata": {
    "papermill": {
     "duration": 0.002433,
     "end_time": "2022-11-12T00:35:11.003424",
     "exception": false,
     "start_time": "2022-11-12T00:35:11.000991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "12. Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the algorithm, you should probably reduce the dataset’s dimensionality (e.g., use PCA, preserving 99% of the variance). Use the model to generate some new faces (using the sample() method), and visualize them (if you used PCA, you will need to use its inverse_transform() method). Try to modify some images (e.g., rotate, ﬂip, darken) and see if the model can detect the anomalies (i.e., compare the output of the score_samples() method for normal images and for anomalies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9134e7",
   "metadata": {
    "papermill": {
     "duration": 0.002561,
     "end_time": "2022-11-12T00:35:11.008778",
     "exception": false,
     "start_time": "2022-11-12T00:35:11.006217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "13. Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the Olivetti faces dataset and reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction error for each image. Next, take some of the modiﬁed images you built in the previous exercise and look at their reconstruction error: notice how much larger it is. If you plot a reconstructed image, you will see why: it tries to reconstruct a normal face."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.086008,
   "end_time": "2022-11-12T00:35:13.634144",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-12T00:35:02.548136",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
