{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4171ff",
   "metadata": {
    "papermill": {
     "duration": 0.003884,
     "end_time": "2022-12-04T16:47:29.023745",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.019861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. What is the problem that Glorot initialization and He initialization aim to ﬁx?\n",
    "\n",
    "    The gradient vanishing and explosion problems: a way to significantly alleviate the unstable gradients problem. They point out that we need the signal to ﬂow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don’t want the signal to die out, nor do we want it to explode and saturate.\n",
    "    \n",
    "    Glorot initialization and He initialization were designed to make the output standard deviation as close as possible to the input standard deviation, at least at the beginning of training. This reduces the vanishing/exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51dfa3",
   "metadata": {
    "papermill": {
     "duration": 0.002674,
     "end_time": "2022-12-04T16:47:29.029684",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.027010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "    No, all weights should be sampled independently; they should not all have the same initial value. One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent), and backpropagation will be unable to break it. Concretely, this means that all the neurons in any given layer will always have the same weights. It's like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e4e6b",
   "metadata": {
    "papermill": {
     "duration": 0.002632,
     "end_time": "2022-12-04T16:47:29.035274",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.032642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "    It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights, and that's OK too; it does not make much difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4739c3d",
   "metadata": {
    "papermill": {
     "duration": 0.002728,
     "end_time": "2022-12-04T16:47:29.040890",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.038162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. In which cases would you want to use each of the activation functions we discussed in this chapter?\n",
    "\n",
    "    ReLU remains a good default for simple tasks: it’s often just as good as the more sophisticated activation functions, plus it’s very fast to compute, and many libraries and hardware accelerators provide ReLUspeciﬁc optimizations. ReLU is usually a good default for the hidden layers, as it is fast and yields good results. Its ability to output precisely zero can also be useful in some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementations as well as from hardware acceleration. \n",
    "    \n",
    "    If you care a lot about runtime latency, then you may prefer leaky ReLU, or parametrized leaky ReLU for more complex tasks. The leaky ReLU variants of ReLU can improve the model's quality without hindering its speed too much compared to ReLU.\n",
    "    \n",
    "    For large neural nets and more complex problems, GLU, Swish and Mish can give you a slightly higher quality model, but they have a computational cost. Swish is probably a better default for more complex tasks, and you can even try parametrized Swish with a learnable β parameter for the most complex tasks. Mish may give you slightly better results, but it requires a bit more compute. \n",
    "    \n",
    "    For deep MLPs, give SELU a try, but make sure to respect the constraints listed earlier. If you have spare time and computing power, you can use cross-validation to evaluate other activation functions as well.\n",
    "    \n",
    "    The hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number in a fixed range (by default between –1 and 1), but nowadays it is not used much in hidden layers, except in recurrent nets.\n",
    "      \n",
    "    The sigmoid activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification), but it is rarely used in hidden layers (there are exceptions—for example, for the coding layer of variational autoencoders; see Chapter 17).\n",
    "\n",
    "    The softplus activation function is useful in the output layer when you need to ensure that the output will always be positive. The softmax activation function is useful in the output layer to estimate probabilities for mutually exclusive classes, but it is rarely (if ever) used in hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b6f76",
   "metadata": {
    "papermill": {
     "duration": 0.002594,
     "end_time": "2022-12-04T16:47:29.046336",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.043742",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "    Too much friction which will take too long to converge.\n",
    "    \n",
    "    If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99902ed7",
   "metadata": {
    "papermill": {
     "duration": 0.002561,
     "end_time": "2022-12-04T16:47:29.051696",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.049135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "6. Name three ways you can produce a sparse model.\n",
    "\n",
    "    - Set the tiny weights to 0\n",
    "    - Apply strong l1 regularization\n",
    "    - Use the pruning API in the TensorFlow Model Optimization Toolkit (TF-MOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c351f7e",
   "metadata": {
    "papermill": {
     "duration": 0.002607,
     "end_time": "2022-12-04T16:47:29.057148",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.054541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC dropout?\n",
    "\n",
    "    Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. \n",
    "    \n",
    "    MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor of 10 or more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec14ae",
   "metadata": {
    "papermill": {
     "duration": 0.002592,
     "end_time": "2022-12-04T16:47:29.062572",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.059980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "    a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the Swish activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b5ba4f",
   "metadata": {
    "papermill": {
     "duration": 0.002587,
     "end_time": "2022-12-04T16:47:29.068055",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.065468",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with tf.keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac76c482",
   "metadata": {
    "papermill": {
     "duration": 0.002734,
     "end_time": "2022-12-04T16:47:29.073744",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.071010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    c. Now try adding batch normalization and compare the learning curves: is it converging faster than before? Does it produce a better model? How does it aﬀect training speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee10c28",
   "metadata": {
    "papermill": {
     "duration": 0.002666,
     "end_time": "2022-12-04T16:47:29.079310",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.076644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    d. Try replacing batch normalization with SELU, and make the necessary adjustments to ensure the network selfnormalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea39295",
   "metadata": {
    "papermill": {
     "duration": 0.00255,
     "end_time": "2022-12-04T16:47:29.084759",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.082209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC dropout. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc4fdf",
   "metadata": {
    "papermill": {
     "duration": 0.002694,
     "end_time": "2022-12-04T16:47:29.090293",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.087599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    f. Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3cfde",
   "metadata": {
    "papermill": {
     "duration": 0.002695,
     "end_time": "2022-12-04T16:47:29.095845",
     "exception": false,
     "start_time": "2022-12-04T16:47:29.093150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.940193,
   "end_time": "2022-12-04T16:47:29.821060",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-12-04T16:47:20.880867",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
