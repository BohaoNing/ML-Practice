{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b2d04d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004524,
     "end_time": "2022-11-27T01:05:02.671453",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.666929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. Draw an ANN using the original artiﬁcial neurons (like the ones in Figure 10-3) that computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A ∧ ¬ B) ∨ (¬ A ∧ B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b479f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002761,
     "end_time": "2022-11-27T01:05:02.679422",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.676661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. Why is it generally preferable to use a logistic regression classiﬁer rather than a classic perceptron (i.e., a single layer of threshold logic units trained using the perceptron training algorithm)? How can you tweak a perceptron to make it equivalent to a logistic regression classiﬁer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b294410",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.004103,
     "end_time": "2022-11-27T01:05:02.687692",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.683589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. Why was the sigmoid activation function a key ingredient in training the ﬁrst MLPs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938eb584",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003545,
     "end_time": "2022-11-27T01:05:02.695057",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.691512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "5. Name three popular activation functions. Can you draw them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28016abe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.00356,
     "end_time": "2022-11-27T01:05:02.703881",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.700321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "6. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artiﬁcial neurons, and ﬁnally one output layer with 3 artiﬁcial neurons. All artiﬁcial neurons use the ReLU activation function.\n",
    "\n",
    "    a. What is the shape of the input matrix X?\n",
    "\n",
    "    b. What are the shapes of the hidden layer’s weight matrix W<sub>h</sub> and bias vector b<sub>h</sub> ?\n",
    "\n",
    "    c. What are the shapes of the output layer’s weight matrix W<sub>o</sub> and bias vector b<sub>o</sub> ?\n",
    "\n",
    "    d. What is the shape of the network’s output matrix Y?\n",
    "\n",
    "    e. Write the equation that computes the network’s output matrix Y as a function of X, W<sub>h</sub>, b<sub>h</sub>, W<sub>o</sub>, and b<sub>o</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a611e289",
   "metadata": {
    "papermill": {
     "duration": 0.002835,
     "end_time": "2022-11-27T01:05:02.709872",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.707037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "7. How many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, and which activation function should you use? What about for getting your network to predict housing prices, as in Chapter 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6a83d",
   "metadata": {
    "papermill": {
     "duration": 0.002773,
     "end_time": "2022-11-27T01:05:02.715687",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.712914",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "8. What is backpropagation and how does it work? What is the diﬀerence between backpropagation and reversemode autodiﬀ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02ae0d",
   "metadata": {
    "papermill": {
     "duration": 0.003263,
     "end_time": "2022-11-27T01:05:02.723148",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.719885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP overﬁts the training data, how could you tweak these hyperparameters to try to solve the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafe666",
   "metadata": {
    "papermill": {
     "duration": 0.00262,
     "end_time": "2022-11-27T01:05:02.728798",
     "exception": false,
     "start_time": "2022-11-27T01:05:02.726178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "10. Train a deep MLP on the MNIST dataset (you can load it using tf.keras.datasets.mnist.load_data()). See if you can get over 98% accuracy by manually tuning the hyperparameters. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, and ﬁnding the point where the loss shoots up). Next, try tuning the hyperparameters using Keras Tuner with all the bells and whistles—save checkpoints, use early stopping, and plot learning curves using TensorBoard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.807694,
   "end_time": "2022-11-27T01:05:03.562136",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-27T01:04:51.754442",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
