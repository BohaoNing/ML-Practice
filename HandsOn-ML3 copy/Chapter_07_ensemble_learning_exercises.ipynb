{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed92f3a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003151,
     "end_time": "2022-11-02T01:07:15.502522",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.499371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. If you have trained ﬁve diﬀerent models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "\n",
    "        Yes, with voing or stacking algorithms.\n",
    "        It works better if the models are very different. It is even better if they are trained on different training instances (that's the whole point of bagging and pasting ensembles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089674c6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002026,
     "end_time": "2022-11-02T01:07:15.507578",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.505552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. What is the diﬀerence between hard and soft voting classiﬁers?\n",
    "\n",
    "        Hard voting takes majority vote. Soft voting takes an average estimated class probability for each class and picks the class with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42851e96",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001883,
     "end_time": "2022-11-02T01:07:15.511661",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.509778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "\n",
    "\n",
    "    It is quite possible to speed up training of a bagging ensemble by distributing it across multiple servers, since each predictor in the ensemble is independent of the others. The same goes for pasting ensembles and Random Forests, for the same reason. \n",
    "    \n",
    "    However, each predictor in a boosting ensemble is built based on the previous predictor, so training is necessarily sequential, and you will not gain anything by distributing training across multiple servers. \n",
    "    \n",
    "    Regarding stacking ensembles, all the predictors in a given layer are independent of each other, so they can be trained in parallel on multiple servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f8fcb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001898,
     "end_time": "2022-11-02T01:07:15.515687",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.513789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. What is the beneﬁt of out-of-bag evaluation?\n",
    "\n",
    "        It gives a realistic accuracy score on \"unseen\" data and thus do not require a validation set anymore.\n",
    "        \n",
    "        Thus, you have more instances available for training, and your ensemble can perform slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7ed75",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001798,
     "end_time": "2022-11-02T01:07:15.519606",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.517808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "5. What makes extra-trees ensembles more random than regular random forests? How can this extra randomness help? Are extra-trees classiﬁers slower or faster than regular random forests?\n",
    "\n",
    "        Because the thresholds are randomly selected. This extra randomness acts like a form of regularization: if a Random Forest overfits the training data, Extra-Trees might perform better. It helps to increase bias and decrease variance, thus prevent from overfitting. \n",
    "        \n",
    "        Moreover, since Extra-Trees don't search for the best possible thresholds, they are much faster to train than Random Forests. However, they are neither faster nor slower than Random Forests when making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202bb16",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001783,
     "end_time": "2022-11-02T01:07:15.523496",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.521713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "6. If your AdaBoost ensemble underﬁts the training data, which hyperparameters should you tweak, and how?\n",
    "\n",
    "        Decrease learning rate. Increase the number of estimators. Reduce the regularization of the base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9252040",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001823,
     "end_time": "2022-11-02T01:07:15.527321",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.525498",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "7. If your gradient boosting ensemble overﬁts the training set, should you increase or decrease the learning rate?\n",
    "\n",
    "        Increase. You could also use early stopping to find the right number of predictors (you probably have too many)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2799b30",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001811,
     "end_time": "2022-11-02T01:07:15.531122",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.529311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "8. Load the MNIST dataset (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classiﬁers, such as a random forest classiﬁer, an extra-trees classiﬁer, and an SVM classiﬁer. Next, try to combine them into an ensemble that outperforms each individual classiﬁer on the validation set, using soft or hard voting. Once you have found one, try it on the test set. How much better does it perform compared to the individual classiﬁers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a69a3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.001801,
     "end_time": "2022-11-02T01:07:15.534918",
     "exception": false,
     "start_time": "2022-11-02T01:07:15.533117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "9. Run the individual classiﬁers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classiﬁers for an image, and the target is the image’s class. Train a classiﬁer on this new training set. Congratulations—you have just trained a blender, and together with the classiﬁers it forms a stacking ensemble! Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classiﬁers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classiﬁer you trained earlier? Now try again using a StackingClassifier instead. Do you get better performance? If so, why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.108589,
   "end_time": "2022-11-02T01:07:16.157714",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-02T01:07:07.049125",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
